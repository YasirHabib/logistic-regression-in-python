# Section 3, Lecture 15
# Baye's theorem: p(Y/X) = p(X/Y) * p(Y) / p(X)
# p(Y/X) = posterior
# p(X/Y) = likelihood -> gaussian -> calculated by taking data from each class & getting their means and covariances
# p(Y) = prior -> maximum likelihood estimate -> e.g p(Y=1) = no of times Y=1 appeared / total no of samples

# wT = (meu1T - meu0T)(inverse of covariance matrix)
# b = 0.5 * (meu0T inverse of covariance matrix)(meu0) - 0.5 * (meu1T inverse of covariance matrix) (meu1) - ln(alpha / 1-alpha)
    # alpha = p(Y=0); 1 - alpha = p(Y=1)
	# meu0 = mean of first gaussian; meu1 = mean of second gaussian
	
# Section 3, Lecture 17
# In logistic regression, error can't be gaussian distributed because the output is between 0 & 1 and the targets are only 0 & 1.
# We want the error to be 0 when there is no error & we want it to get larger, the more incorrect we are.

# Hence we use the cross error entropy function -> j = -{tlog(y) + (1-t)log(1-y)}
	# t = target (this will be EITHER 0 or 1)
	# y = output of logistic (this will be BETWEEN 0 & 1) -> log(y) = -infinity to 0 -> multiplying with minus = 0 to +infinity
	
# Since we want total error, we sum up all the individual errors.
# _______________________________________________________________________________________________________________________________________
# demonstrates how to calculate the cross entropy error function in numpy.

import numpy as np
import matplotlib.pyplot as plt

N = 100
D = 2

X = np.random.randn(N, D)

# center the first 50 points at (-2,-2)
#X[:50,:] = X[:50,:] - 2*np.ones((50,D)))
X[:50,0] = X[:50,0] - 2
X[:50,1] = X[:50,1] - 2

# center the last 50 points at (2, 2)
#X[50:,:] = X[50:,:] + 2*np.ones((50,D))
X[50:,0] = X[50:,0] + 2
X[50:,1] = X[50:,1] + 2

# labels: first 50 are 0, last 50 are 1
Target = np.array([0]*50 + [1]*50)

# add bias term
X = np.column_stack([X, np.ones(N)])

# randomly initialize the weights
w = np.random.randn(D + 1)

# calculate the model output
z = np.dot(X, w)

def sigmoid(z):
    return 1 / (1 + np.exp(-z))

predictions = sigmoid(z)

# calculate the cross-entropy error
def cross_entropy(Target, Y):
	j = 0
	for x in range(N):
		j = j + -(Target[x]*np.log(Y[x]) + (1-Target[x])*np.log(1-Y[x]))
	return j
	
print(cross_entropy(Target, predictions))

#________________________________________________________________________________________________________________________________
# try it with our closed-form solution

# wT = (meu1T - meu0T)(inverse of covariance matrix)
meu1 = np.array([2, 2])
meu1T = np.transpose(meu1)

meu0 = np.array([-2, -2])
meu0T = np.transpose(meu0)
# we know the covariance matrix is below. It has 1's in diagonal since the variance of both gaussians is a 1 because they are generated by
# np.random.randn. It has 0's in off-diagonals since they are uncorrelated & independent.
cov = np.array([[1, 0], [0, 1]])
cov_inv = np.linalg.inv(cov)
wT = np.dot((meu1T - meu0T), cov_inv)

# b = 0.5 * (meu0T inverse of covariance matrix)(meu0) - 0.5 * (meu1T inverse of covariance matrix) (meu1) - ln(alpha / 1-alpha)
alpha = 0.5
b = 0.5 * np.dot(np.dot(meu0T, cov_inv), meu0) - 0.5 * np.dot(np.dot(meu1T, cov_inv), meu1) - np.log(alpha / (1 - alpha))

wT = np.append(wT, [b])

z = np.dot(X, wT) + b

predictions = sigmoid(z)

print(cross_entropy(Target, predictions))